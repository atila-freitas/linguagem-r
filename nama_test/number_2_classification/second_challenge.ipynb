{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "import io\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 4342\n",
      "Class 1: 3271\n",
      "Proportion: 1.33 : 1\n"
     ]
    }
   ],
   "source": [
    "#import file of training ==> almost balanced dataset\n",
    "\n",
    "metrics_results = {}\n",
    "\n",
    "training_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "size_of_training = len(training_df.index)\n",
    "\n",
    "classlabel_count = training_df.target.value_counts()\n",
    "print('Class 0:', classlabel_count[0])\n",
    "print('Class 1:', classlabel_count[1])\n",
    "print('Proportion:', round(classlabel_count[0] / classlabel_count[1], 2), ': 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "len(test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load language and make functions to help to prepare\n",
    "nlp_lemma = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Convert to array\n",
    "def convert_text_to_array(text):\n",
    "    text_converted = nlp_lemma(text)\n",
    "    \n",
    "    return [each_word for each_word in text_converted]\n",
    "\n",
    "# Get token and lemmatization\n",
    "def get_token_and_lemma(text):\n",
    "    return [each_word.lemma_ for each_word in text]\n",
    "\n",
    "# remove stop words and not users citation\n",
    "def remove_stop_words(tokens):\n",
    "    return [each_word for each_word in tokens \\\n",
    "            if not nlp_lemma.vocab[each_word].is_stop \\\n",
    "            and nlp_lemma.vocab[each_word].is_alpha \\\n",
    "            and each_word[:1] != \"@\"]\n",
    "\n",
    "def fit_and_make_submission_file(train_vectors, test_vectors, target, clf, filename):\n",
    "    clf.fit(train_vectors, target)\n",
    "    sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "    sample_submission[\"target\"] = clf.predict(test_vectors)\n",
    "    sample_submission.to_csv(\"submission_files/\"+filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tokens in the text and remove stop words\n",
    "training_df['tokens'] = training_df['text'].apply(lambda x: convert_text_to_array(x))\n",
    "training_df['tokens'] = training_df['tokens'].apply(lambda x: get_token_and_lemma(x))\n",
    "training_df['tokens'] = training_df['tokens'].apply(lambda x: remove_stop_words(x))\n",
    "\n",
    "test_df['tokens'] = test_df['text'].apply(lambda x: convert_text_to_array(x))\n",
    "test_df['tokens'] = test_df['tokens'].apply(lambda x: get_token_and_lemma(x))\n",
    "test_df['tokens'] = test_df['tokens'].apply(lambda x: remove_stop_words(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[deed, Reason, earthquake, ALLAH, forgive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[got, send, photo, Ruby, Alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                             tokens  \n",
       "0       1         [deed, Reason, earthquake, ALLAH, forgive]  \n",
       "1       1      [forest, fire, near, La, Ronge, Sask, Canada]  \n",
       "2       1  [resident, ask, shelter, place, notify, office...  \n",
       "3       1  [people, receive, wildfire, evacuation, order,...  \n",
       "4       1  [got, send, photo, Ruby, Alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetorizar utilizando bag of words de 1 palavra\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "metrics_results = {}\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=dummy, preprocessor=dummy, ngram_range=(1,1))\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(training_df[\"tokens\"])\n",
    "test_vectors = vectorizer.transform(test_df[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle - Score 0.77903 - bag of words - ngram(1,1)\n",
    "# Cross validation - F1 score - 0.5671622153365451 +/- 0.06033706267666568\n",
    "# Cross validation - F1 score - Max - 0.68197474\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, training_df[\"target\"], cv=7, scoring=\"f1\")\n",
    "\n",
    "metrics_results[\"bow_1x1_ridge\"] = {\"mean\": scores.mean(), \"std\": scores.std(), \"max\" : scores.max()}\n",
    "\n",
    "fit_and_make_submission_file(train_vectors, test_vectors, training_df[\"target\"], clf, \"bow_1x1_ridge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5804462053066946\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(max_iter=1000, tol=10)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, training_df[\"target\"], cv=7, scoring=\"f1\")\n",
    "\n",
    "metrics_results[\"bow_1x1_sgdc\"] = {\"mean\": scores.mean(), \"std\": scores.std(), \"max\" : scores.max()}\n",
    "\n",
    "fit_and_make_submission_file(train_vectors, test_vectors, training_df[\"target\"], clf, \"bow_1x1_sgdc.csv\")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation - F1 score - 0.5930793845835153 +/- 0.056694945830674255\n",
    "\n",
    "clf = SVC(gamma=0.01, C=10)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, training_df[\"target\"], cv=7, scoring=\"f1\")\n",
    "\n",
    "metrics_results[\"bow_1x1_svc\"] = {\"mean\": scores.mean(), \"std\": scores.std(), \"max\" : scores.max()}\n",
    "\n",
    "fit_and_make_submission_file(train_vectors, test_vectors, training_df[\"target\"], clf, \"bow_1x1_svc.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetorizar utilizando tfidf\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=dummy, preprocessor=dummy)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(training_df[\"tokens\"])\n",
    "test_vectors = vectorizer.transform(test_df[\"tokens\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63878327 0.53802497 0.53488372 0.55529954 0.54681648 0.54100367\n",
      " 0.69634703]\n",
      "0.5787369550029594 +/- 0.058566557673596066\n"
     ]
    }
   ],
   "source": [
    "# Cross validation - F1 score - 0.5767244438742305 +/- 0.06304273823463757\n",
    "# Cross validation - F1 score - Max - 0.70842825\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, training_df[\"target\"], cv=7, scoring=\"f1\")\n",
    "print(scores)\n",
    "print(str(scores.mean())  + \" +/- \" + str(scores.std()))\n",
    "\n",
    "metrics_results[\"tfidf_ridge\"] = {\"mean\": scores.mean(), \"std\": scores.std(), \"max\" : scores.max()}\n",
    "\n",
    "fit_and_make_submission_file(train_vectors, test_vectors, training_df[\"target\"], clf, \"tfidf_ridge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5907146866067939\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(max_iter=1000, tol=10)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, training_df[\"target\"], cv=7, scoring=\"f1\")\n",
    "\n",
    "metrics_results[\"tfidf_sgdc\"] = {\"mean\": scores.mean(), \"std\": scores.std(), \"max\" : scores.max()}\n",
    "\n",
    "fit_and_make_submission_file(train_vectors, test_vectors, training_df[\"target\"], clf, \"tfidf_sgdc.csv\")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma=0.01, C=10)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, training_df[\"target\"], cv=7, scoring=\"f1\")\n",
    "\n",
    "metrics_results[\"tfidf_svc\"] = {\"mean\": scores.mean(), \"std\": scores.std(), \"max\" : scores.max()}\n",
    "\n",
    "fit_and_make_submission_file(train_vectors, test_vectors, training_df[\"target\"], clf, \"tfidf_svc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5934339539170913\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, training_df[\"target\"], cv=7, scoring=\"f1\")\n",
    "\n",
    "metrics_results[\"tfidf_logistic\"] = {\"mean\": scores.mean(), \"std\": scores.std(), \"max\" : scores.max()}\n",
    "\n",
    "fit_and_make_submission_file(train_vectors, test_vectors, training_df[\"target\"], clf, \"tfidf_logistic.csv\")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = LinearSVC()\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, training_df[\"target\"], cv=7, scoring=\"f1\")\n",
    "\n",
    "metrics_results[\"tfidf_linearsvc\"] = {\"mean\": scores.mean(), \"std\": scores.std(), \"max\" : scores.max()}\n",
    "\n",
    "fit_and_make_submission_file(train_vectors, test_vectors, training_df[\"target\"], clf, \"tfidf_linearsvc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=dummy, preprocessor=dummy, ngram_range=(1,2))\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(training_df[\"tokens\"])\n",
    "test_vectors = vectorizer.transform(test_df[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = RidgeClassifier()\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, training_df[\"target\"], cv=7, scoring=\"f1\")\n",
    "\n",
    "metrics_results[\"bow_1x2_ridge\"] = {\"mean\": scores.mean(), \"std\": scores.std(), \"max\" : scores.max()}\n",
    "\n",
    "fit_and_make_submission_file(train_vectors, test_vectors, training_df[\"target\"], clf, \"bow_1x2_ridge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, training_df[\"target\"], cv=7, scoring=\"f1\")\n",
    "\n",
    "metrics_results[\"bow_1x2_logistic\"] = {\"mean\": scores.mean(), \"std\": scores.std(), \"max\" : scores.max()}\n",
    "\n",
    "fit_and_make_submission_file(train_vectors, test_vectors, training_df[\"target\"], clf, \"bow_1x2_logistic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = LinearSVC()\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, training_df[\"target\"], cv=7, scoring=\"f1\")\n",
    "\n",
    "metrics_results[\"bow_1x2_linearsvc\"] = {\"mean\": scores.mean(), \"std\": scores.std(), \"max\" : scores.max()}\n",
    "\n",
    "fit_and_make_submission_file(train_vectors, test_vectors, training_df[\"target\"], clf, \"bow_1x2_linearsvc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NuSVC()\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, training_df[\"target\"], cv=7, scoring=\"f1\")\n",
    "\n",
    "metrics_results[\"bow_1x2_nusvc\"] = {\"mean\": scores.mean(), \"std\": scores.std(), \"max\" : scores.max()}\n",
    "\n",
    "fit_and_make_submission_file(train_vectors, test_vectors, training_df[\"target\"], clf, \"bow_1x2_nusvc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SGDClassifier(max_iter=1000, tol=1000000)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, training_df[\"target\"], cv=7, scoring=\"f1\")\n",
    "\n",
    "metrics_results[\"bow_1x2_sgdc\"] = {\"mean\": scores.mean(), \"std\": scores.std(), \"max\" : scores.max()}\n",
    "\n",
    "fit_and_make_submission_file(train_vectors, test_vectors, training_df[\"target\"], clf, \"bow_1x2_sgdc.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tfidf_logistic</th>\n",
       "      <td>0.593434</td>\n",
       "      <td>0.050360</td>\n",
       "      <td>0.687169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bow_1x1_svc</th>\n",
       "      <td>0.592198</td>\n",
       "      <td>0.058246</td>\n",
       "      <td>0.709898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_sgdc</th>\n",
       "      <td>0.590715</td>\n",
       "      <td>0.055751</td>\n",
       "      <td>0.701604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_linearsvc</th>\n",
       "      <td>0.585003</td>\n",
       "      <td>0.055227</td>\n",
       "      <td>0.698554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bow_1x2_logistic</th>\n",
       "      <td>0.582876</td>\n",
       "      <td>0.058849</td>\n",
       "      <td>0.695556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bow_1x1_sgdc</th>\n",
       "      <td>0.580446</td>\n",
       "      <td>0.056624</td>\n",
       "      <td>0.693092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_ridge</th>\n",
       "      <td>0.578737</td>\n",
       "      <td>0.058567</td>\n",
       "      <td>0.696347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bow_1x2_sgdc</th>\n",
       "      <td>0.576534</td>\n",
       "      <td>0.071120</td>\n",
       "      <td>0.698630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bow_1x2_linearsvc</th>\n",
       "      <td>0.566132</td>\n",
       "      <td>0.057814</td>\n",
       "      <td>0.668224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bow_1x1_ridge</th>\n",
       "      <td>0.565406</td>\n",
       "      <td>0.061901</td>\n",
       "      <td>0.665148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bow_1x2_nusvc</th>\n",
       "      <td>0.564280</td>\n",
       "      <td>0.062968</td>\n",
       "      <td>0.682095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bow_1x2_ridge</th>\n",
       "      <td>0.560292</td>\n",
       "      <td>0.063476</td>\n",
       "      <td>0.665094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_svc</th>\n",
       "      <td>0.407381</td>\n",
       "      <td>0.089967</td>\n",
       "      <td>0.571006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean       std       max\n",
       "tfidf_logistic     0.593434  0.050360  0.687169\n",
       "bow_1x1_svc        0.592198  0.058246  0.709898\n",
       "tfidf_sgdc         0.590715  0.055751  0.701604\n",
       "tfidf_linearsvc    0.585003  0.055227  0.698554\n",
       "bow_1x2_logistic   0.582876  0.058849  0.695556\n",
       "bow_1x1_sgdc       0.580446  0.056624  0.693092\n",
       "tfidf_ridge        0.578737  0.058567  0.696347\n",
       "bow_1x2_sgdc       0.576534  0.071120  0.698630\n",
       "bow_1x2_linearsvc  0.566132  0.057814  0.668224\n",
       "bow_1x1_ridge      0.565406  0.061901  0.665148\n",
       "bow_1x2_nusvc      0.564280  0.062968  0.682095\n",
       "bow_1x2_ridge      0.560292  0.063476  0.665094\n",
       "tfidf_svc          0.407381  0.089967  0.571006"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records(metrics_results).transpose().sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59230 words total, with a vocabulary size of 14387\n",
      "Max sentence length is 21\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "word2vec = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "all_words = [word for tokens in training_df[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in training_df[\"tokens\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22700 unique tokens.\n",
      "(22701, 300)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "\n",
    "VALIDATION_SPLIT=.2\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(training_df[\"text\"].tolist())\n",
    "sequences = tokenizer.texts_to_sequences(training_df[\"text\"].tolist())\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "cnn_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(training_df[\"target\"]))\n",
    "\n",
    "indices = np.arange(cnn_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "cnn_data = cnn_data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * cnn_data.shape[0])\n",
    "\n",
    "embedding_weights = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in word_index.items():\n",
    "    embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "\n",
    "    preds = Dense(labels_index, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = cnn_data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = cnn_data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "model = ConvNet(embedding_weights, MAX_SEQUENCE_LENGTH, len(word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(training_df[\"target\"].unique())), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "48/48 [==============================] - 34s 676ms/step - loss: 0.6667 - acc: 0.6319 - val_loss: 0.5089 - val_acc: 0.7661\n",
      "Epoch 2/3\n",
      "48/48 [==============================] - 26s 549ms/step - loss: 0.4681 - acc: 0.7841 - val_loss: 0.4634 - val_acc: 0.7792\n",
      "Epoch 3/3\n",
      "48/48 [==============================] - 44s 913ms/step - loss: 0.4062 - acc: 0.8275 - val_loss: 0.5202 - val_acc: 0.7444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x252b05d50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(test_df[\"text\"].tolist())\n",
    "cnn_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "results = model.predict(cnn_data).tolist()\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_submission[\"target\"] = [ 0 if i[0] > i[1] else 1 for i in results]\n",
    "sample_submission.to_csv(\"submission_files/cnn_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
